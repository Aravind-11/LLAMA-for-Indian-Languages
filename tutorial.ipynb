{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.47.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.2.0-cp39-cp39-macosx_10_10_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.2-cp39-cp39-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp39-cp39-macosx_10_9_x86_64.whl (257 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.1/257.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.47.0-cp39-cp39-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Downloading kiwisolver-1.4.5-cp39-cp39-macosx_10_9_x86_64.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp39-cp39-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.47.0 importlib-resources-6.1.1 kiwisolver-1.4.5 matplotlib-3.8.2 pillow-10.2.0 pyparsing-3.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1695832228254,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "ktEA7qXmwdUM"
   },
   "outputs": [],
   "source": [
    "# presentation layer code\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "def genai_app_arch():\n",
    "  mm(\"\"\"\n",
    "  flowchart TD\n",
    "    A[Users] --> B(Applications e.g. mobile, web)\n",
    "    B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n",
    "    B -- optional --> E(Frameworks e.g. LangChain)\n",
    "    C-->|User Input|D[Llama 2]\n",
    "    D-->|Model Output|C\n",
    "    E --> C\n",
    "    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def rag_arch():\n",
    "  mm(\"\"\"\n",
    "  flowchart TD\n",
    "    A[User Prompts] --> B(Frameworks e.g. LangChain)\n",
    "    B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n",
    "    B -->|API|D[Llama 2]\n",
    "    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def llama2_family():\n",
    "  mm(\"\"\"\n",
    "  graph LR;\n",
    "      llama-2 --> llama-2-7b\n",
    "      llama-2 --> llama-2-13b\n",
    "      llama-2 --> llama-2-70b\n",
    "      llama-2-7b --> llama-2-7b-chat\n",
    "      llama-2-13b --> llama-2-13b-chat\n",
    "      llama-2-70b --> llama-2-70b-chat\n",
    "      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def apps_and_llms():\n",
    "  mm(\"\"\"\n",
    "  graph LR;\n",
    "    users --> apps\n",
    "    apps --> frameworks\n",
    "    frameworks --> platforms\n",
    "    platforms --> Llama 2\n",
    "    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create a text widget\n",
    "API_KEY = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='API_KEY:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def md(t):\n",
    "  display(Markdown(t))\n",
    "\n",
    "def bot_arch():\n",
    "  mm(\"\"\"\n",
    "  graph LR;\n",
    "  user --> prompt\n",
    "  prompt --> i_safety\n",
    "  i_safety --> context\n",
    "  context --> Llama_2\n",
    "  Llama_2 --> output\n",
    "  output --> o_safety\n",
    "  i_safety --> memory\n",
    "  o_safety --> memory\n",
    "  memory --> context\n",
    "  o_safety --> user\n",
    "  classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def fine_tuned_arch():\n",
    "  mm(\"\"\"\n",
    "  graph LR;\n",
    "      Custom_Dataset --> Pre-trained_Llama\n",
    "      Pre-trained_Llama --> Fine-tuned_Llama\n",
    "      Fine-tuned_Llama --> RLHF\n",
    "      RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n",
    "      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def load_data_faiss_arch():\n",
    "  mm(\"\"\"\n",
    "  graph LR;\n",
    "      documents --> textsplitter\n",
    "      textsplitter --> embeddings\n",
    "      embeddings --> vectorstore\n",
    "      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n",
    "\n",
    "def mem_context():\n",
    "  mm(\"\"\"\n",
    "      graph LR\n",
    "      context(text)\n",
    "      user_prompt --> context\n",
    "      instruction --> context\n",
    "      examples --> context\n",
    "      memory --> context\n",
    "      context --> tokenizer\n",
    "      tokenizer --> embeddings\n",
    "      embeddings --> LLM\n",
    "      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
    "  \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGPSI3M5PGTi"
   },
   "source": [
    "### **1.1 - What is Llama 2?**\n",
    "\n",
    "* State of the art (SOTA), Open Source LLM\n",
    "* 7B, 13B, 70B\n",
    "* Pretrained + Chat\n",
    "* Choosing model: Size, Quality, Cost, Speed\n",
    "* [Research paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n",
    "\n",
    "* [Responsible use guide](https://ai.meta.com/llama/responsible-use-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1695832233087,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "OXRCC7wexZXd",
    "outputId": "1feb1918-df4b-4cec-d09e-ffe55c12090b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgZ3JhcGggTFI7CiAgICAgIGxsYW1hLTIgLS0+IGxsYW1hLTItN2IKICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi03MGIKICAgICAgbGxhbWEtMi03YiAtLT4gbGxhbWEtMi03Yi1jaGF0CiAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgIGxsYW1hLTItNzBiIC0tPiBsbGFtYS0yLTcwYi1jaGF0CiAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama2_family()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd54g0OHuqBY"
   },
   "source": [
    "##**2 - Using Llama 2**\n",
    "\n",
    "In this notebook, we are going to access [Llama 13b chat model](https://replicate.com/meta/llama-2-13b-chat) using hosted API from Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3YGMDJidHtH"
   },
   "source": [
    "### **2.1 - Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VhN6hXwx7FCp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies and initialize\n",
    "%pip install -qU \\\n",
    "    replicate \\\n",
    "    langchain \\\n",
    "    sentence_transformers \\\n",
    "    pdf2image \\\n",
    "    pdfminer \\\n",
    "    pdfminer.six \\\n",
    "    unstructured \\\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z8Y8qjEjmg50"
   },
   "outputs": [],
   "source": [
    "# model url on Replicate platform that we will use for inferencing\n",
    "# We will use llama 23b chat model hosted on replicate server ()\n",
    "\n",
    "llama2_13b = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8hkWpqWD28ho"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# We will use Replicate hosted cloud environment\n",
    "# Obtain Replicate API key → https://replicate.com/account/api-tokens)\n",
    "\n",
    "# enter your replicate api token\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "# alternatively, you can also store the tokens in environment variables and load it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate = replicate.client.Client(api_token='r8_6IZFZz05PxzfSs5I7VzOSvKwOnFgJzb05KYiw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bVCHZmETk36v"
   },
   "outputs": [],
   "source": [
    "# we will use replicate's hosted api\n",
    "import replicate\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "  output = replicate.run(\n",
    "      llama2_13b,\n",
    "      input={\"prompt\": prompt, \"max_new_tokens\":1000}\n",
    "  )\n",
    "  return \"\".join(output)\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "  output = replicate.run(\n",
    "    llama2_13b,\n",
    "    input={\"system_prompt\": system_prompt,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_new_tokens\":1000}\n",
    "  )\n",
    "  return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Jxq0pmf6L73"
   },
   "source": [
    "### **2.2 - Basic completion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H93zZBIk6tNU"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Oh my llamas! The typical color of a llama is... (drumroll please)... GRAY! That's right, most lamas have a beautiful gray coat with a soft, woolly texture. Some lamas may have white patches on their faces or legs, but overall, gray is the most common color you'll see in these lovable creatures. Now, if you have any other questions about llamas or any other topics, just let me know and I'll do my best to assist you!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = Completion(prompt=\"The typical color of a llama is: \")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StccjUDh6W0Q"
   },
   "source": [
    "### **2.3 - System prompts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRnFogxd6rTc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Brown"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = ChatCompletion(\n",
    "    prompt=\"The typical color of a llama is: \",\n",
    "    system_prompt=\"respond with only one word\"\n",
    "  )\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp4GNa066pYy"
   },
   "source": [
    "### **2.4 - Response formats**\n",
    "* Can support different formatted outputs e.g. text, JSON, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HTN79h4RptgQ"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " {\n",
       "\"answer\": \"The typical color of a llama is brown.\"\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = ChatCompletion(\n",
    "    prompt=\"The typical color of a llama is: \",\n",
    "    system_prompt=\"response in json format\"\n",
    "  )\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWs_s9y-avIT"
   },
   "source": [
    "## **3 - Gen AI Application Architecture**\n",
    "\n",
    "Here is the high-level tech stack/architecture of Generative AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1695832253437,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "j9BGuI-9AOL5",
    "outputId": "72b2613f-a434-4219-f063-52a409af97cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgZmxvd2NoYXJ0IFRECiAgICBBW1VzZXJzXSAtLT4gQihBcHBsaWNhdGlvbnMgZS5nLiBtb2JpbGUsIHdlYikKICAgIEIgLS0+IHxIb3N0ZWQgQVBJfEMoUGxhdGZvcm1zIGUuZy4gQ3VzdG9tLCBIdWdnaW5nRmFjZSwgUmVwbGljYXRlKQogICAgQiAtLSBvcHRpb25hbCAtLT4gRShGcmFtZXdvcmtzIGUuZy4gTGFuZ0NoYWluKQogICAgQy0tPnxVc2VyIElucHV0fERbTGxhbWEgMl0KICAgIEQtLT58TW9kZWwgT3V0cHV0fEMKICAgIEUgLS0+IEMKICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogIA==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genai_app_arch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UlxBtbgys6j"
   },
   "source": [
    "##4 - **Chatbot Architecture**\n",
    "\n",
    "Here are the key components and the information flow in a chatbot.\n",
    "\n",
    "* User Prompts\n",
    "* Input Safety\n",
    "* Llama 2\n",
    "* Output Safety\n",
    "\n",
    "* Memory & Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1695832257063,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "tO5HnB56ys6t",
    "outputId": "f222d35b-626f-4dc1-b7af-a156a0f3d58b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgZ3JhcGggTFI7CiAgdXNlciAtLT4gcHJvbXB0CiAgcHJvbXB0IC0tPiBpX3NhZmV0eQogIGlfc2FmZXR5IC0tPiBjb250ZXh0CiAgY29udGV4dCAtLT4gTGxhbWFfMgogIExsYW1hXzIgLS0+IG91dHB1dAogIG91dHB1dCAtLT4gb19zYWZldHkKICBpX3NhZmV0eSAtLT4gbWVtb3J5CiAgb19zYWZldHkgLS0+IG1lbW9yeQogIG1lbW9yeSAtLT4gY29udGV4dAogIG9fc2FmZXR5IC0tPiB1c2VyCiAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot_arch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4DyTLD5ys6t"
   },
   "source": [
    "### **4.1 - Chat conversation**\n",
    "* LLMs are stateless\n",
    "* Single Turn\n",
    "\n",
    "* Multi Turn (Memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EMM_egWMys6u"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure! The average lifespan of a llama is around 20-30 years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of single turn chat\n",
    "prompt_chat = \"What is the average lifespan of a Llama?\"\n",
    "output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sZ7uVKDYucgi"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure! Here's my answer in a few words:\n",
       "\n",
       "Kangaroos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example without previous context. LLM's are stateless and cannot understand \"they\" without previous context\n",
    "prompt_chat = \"What animal family are they?\"\n",
    "output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQl3wmfbyBQ1"
   },
   "source": [
    "Chat app requires us to send in previous context to LLM to get in valid responses. Below is an example of Multi-turn chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t7SZe5fT3HG3"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure, I'd be happy to help! Llamas are members of the camelid family, which includes other animals like camels and alpacas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of multi-turn chat, with storing previous context\n",
    "prompt_chat = \"\"\"\n",
    "User: What is the average lifespan of a Llama?\n",
    "Assistant: Sure! The average lifespan of a llama is around 20-30 years.\n",
    "User: What animal family are they?\n",
    "\"\"\"\n",
    "output = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moXnmJ_xyD10"
   },
   "source": [
    "### **4.2 - Prompt Engineering**\n",
    "* Prompt engineering refers to the science of designing effective prompts to get desired responses\n",
    "\n",
    "* Helps reduce hallucination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-v-FeZ4ztTB"
   },
   "source": [
    "#### **4.2.1 - In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    " * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any\n",
    "input examples.\n",
    "  2. Few or “N-Shot” Learning - model is performing and behaving based on input examples in user's prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6W71MFNZyRkQ"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Cute"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment: ?\n",
    "'''\n",
    "output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MCQRjf1Y1RYJ"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Neutral"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "\n",
    "prompt = '''\n",
    "Classify: I love Llamas!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw a Gecko.\n",
    "Sentiment:'''\n",
    "\n",
    "output = ChatCompletion(prompt, system_prompt=\"One word response\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8UmdlTmpDZxA"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Luxurious."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# another zero-shot learning\n",
    "prompt = '''\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:'''\n",
    "\n",
    "output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M_EcsUo1zqFD"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Yes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another few-shot learning example with formatted prompt.\n",
    "\n",
    "prompt = '''\n",
    "QUESTION: Llama?\n",
    "ANSWER: Yes\n",
    "QUESTION: Alpaca?\n",
    "ANSWER: Yes\n",
    "QUESTION: Rabbit?\n",
    "ANSWER: No\n",
    "QUESTION: Vicuna?\n",
    "ANSWER:'''\n",
    "\n",
    "output = ChatCompletion(prompt, system_prompt=\"one word response\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbr124Y197xl"
   },
   "source": [
    "#### **4.2.2 - Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Xn8zmLBQzpgj"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure! Here's the answer:\n",
       "\n",
       "Llama has 8 tennis balls now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "'''\n",
    "\n",
    "output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lKNOj79o1Kwu"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure! Here's the solution step by step:\n",
       "\n",
       "1. Llama starts with 5 tennis balls.\n",
       "2. Llama buys 2 more cans of tennis balls, and each can contains 3 tennis balls.\n",
       "3. So, Llama now has 5 + 2 x 3 = 5 + 6 = 11 tennis balls."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain-Of-Thought prompting\n",
    "prompt = '''\n",
    "Llama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n",
    "Let's think step by step.\n",
    "'''\n",
    "\n",
    "output = ChatCompletion(prompt, system_prompt=\"provide short answer\")\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7tDW-AH770Y"
   },
   "source": [
    "### **4.3 - Retrieval Augmented Generation (RAG)**\n",
    "* Prompt Eng Limitations - Knowledge cutoff & lack of specialized data\n",
    "\n",
    "* Retrieval Augmented Generation(RAG) allows us to retrieve snippets of information from external data sources and augment it to the user's prompt to get tailored responses from Llama 2.\n",
    "\n",
    "For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1695832267093,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "Fl1LPltpRQD9",
    "outputId": "4410c9bf-3559-4a05-cebb-a5731bb094c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgZmxvd2NoYXJ0IFRECiAgICBBW1VzZXIgUHJvbXB0c10gLS0+IEIoRnJhbWV3b3JrcyBlLmcuIExhbmdDaGFpbikKICAgIEIgPC0tPiB8RGF0YWJhc2UsIERvY3MsIFhMU3xDW2ZhOmZhLWRhdGFiYXNlIEV4dGVybmFsIERhdGFdCiAgICBCIC0tPnxBUEl8RFtMbGFtYSAyXQogICAgY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNDQ0U2RkYsc3Ryb2tlOiM4NEJDRjUsdGV4dENvbG9yOiMxQzJCMzMsZm9udEZhbWlseTp0cmVidWNoZXQgbXM7CiAg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_arch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJaGMLl_4vYm"
   },
   "source": [
    "#### **4.3.1 - LangChain**\n",
    "LangChain is a framework that helps make it easier to implement RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aoqU3KTcHTWN"
   },
   "outputs": [],
   "source": [
    "# langchain setup\n",
    "from langchain.llms import Replicate\n",
    "# Use the Llama 2 model hosted on Replicate\n",
    "# Temperature: Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value\n",
    "# top_p: When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\n",
    "# max_new_tokens: Maximum number of tokens to generate. A word is generally 2-3 tokens\n",
    "llama_model = Replicate(\n",
    "    model=llama2_13b,\n",
    "    model_kwargs={\"temperature\": 0.75,\"top_p\": 1, \"max_new_tokens\":1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAV2EkZqcruF"
   },
   "outputs": [],
   "source": [
    "# Step 1: load the external data source. In our case, we will load Meta’s “Responsible Use Guide” pdf document.\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://ai.meta.com/static-resource/responsible-use-guide/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Get text splits from document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Use the embedding model\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" # embedding model\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# Step 4: Use vector store to store embeddings\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2l8S5tBxlkc"
   },
   "source": [
    "#### **4.3.2 - LangChain Q&A Retriever**\n",
    "* ConversationalRetrievalChain\n",
    "\n",
    "* Query the Source documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmEhBe3Kiyre"
   },
   "outputs": [],
   "source": [
    "# Query against your own data\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chain = ConversationalRetrievalChain.from_llm(llama_model, vectorstore.as_retriever(), return_source_documents=True)\n",
    "\n",
    "chat_history = []\n",
    "query = \"How is Meta approaching open science in two short sentences?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "md(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CelLHIvoy2Ke"
   },
   "outputs": [],
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"How is it benefiting the world?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "md(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEvefAWIJONx"
   },
   "source": [
    "## **5 - Fine-Tuning Models**\n",
    "\n",
    "* Limitatons of Prompt Eng and RAG\n",
    "* Fine-Tuning Arch\n",
    "* Types (PEFT, LoRA, QLoRA)\n",
    "* Using PyTorch for Pre-Training & Fine-Tuning\n",
    "\n",
    "* Evals + Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1695832272878,
     "user": {
      "displayName": "Amit Sangani",
      "userId": "11552178012079240149"
     },
     "user_tz": 420
    },
    "id": "0a9CvJ8YcTzV",
    "outputId": "56a6d573-a195-4e3c-834d-a3b23485186c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CiAgZ3JhcGggTFI7CiAgICAgIEN1c3RvbV9EYXRhc2V0IC0tPiBQcmUtdHJhaW5lZF9MbGFtYQogICAgICBQcmUtdHJhaW5lZF9MbGFtYSAtLT4gRmluZS10dW5lZF9MbGFtYQogICAgICBGaW5lLXR1bmVkX0xsYW1hIC0tPiBSTEhGCiAgICAgIFJMSEYgLS0+IHxMb3NzOkNyb3NzLUVudHJvcHl8RmluZS10dW5lZF9MbGFtYQogICAgICBjbGFzc0RlZiBkZWZhdWx0IGZpbGw6I0NDRTZGRixzdHJva2U6Izg0QkNGNSx0ZXh0Q29sb3I6IzFDMkIzMyxmb250RmFtaWx5OnRyZWJ1Y2hldCBtczsKICA=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fine_tuned_arch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ioVMNcTesSEk"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
